##
##  node1通过flume接收node2与node3中flume传来的数据，并将其分别发送至hbase与kafka中，配置内容如下
##
a1.sources=r1
a1.channels=kafkaC hdfsC
a1.sinks=kafkaSink hdfsSink

a1.sources.r1.type=avro
a1.sources.r1.channels=kafkaC hdfsC
a1.sources.r1.bind=127.0.0.1
a1.sources.r1.port=5555 
a1.sources.r1.threads=5 

a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.vita.flume.interceptor.AppCollectionInterceptor$Builder

##****************************flume + hdfs******************************

a1.channels.hdfsC.type = memory
a1.channels.hdfsC.capacity = 100000
a1.channels.hdfsC.transactionCapacity = 100

a1.sinks.hdfsSink.channel = hdfsC
a1.sinks.hdfsSink.type = hdfs
a1.sinks.hdfsSink.hdfs.path = /user/centos/applogs/%{logType}/%Y%m/%d/%H%M
a1.sinks.hdfsSink.hdfs.filePrefix = events
a1.sinks.hdfsSink.hdfs.round = true
a1.sinks.hdfsSink.hdfs.roundValue = 30
a1.sinks.hdfsSink.hdfs.roundUnit = second

#不要产生大量小文件
a1.sinks.k1.hdfs.rollInterval = 30
a1.sinks.k1.hdfs.rollSize = 0
a1.sinks.k1.hdfs.rollCount = 0
#控制输出文件是原生文件
a1.sinks.k1.hdfs.fileType = DataStream


#****************************flume + kafka******************************

a1.channels.kafkaC.type=memory
a1.channels.kafkaC.capacity=10000
a1.channels.kafkaC.transactionCapacity=10000
a1.channels.kafkaC.keep-alive=20

a1.sinks.kafkaSink.channel=kafkaC
a1.sinks.kafkaSink.type=com.vita.flume.sink.kafka.ReadAppKafkaSink
a1.sinks.kafkaSink.requiredAcks=1
a1.sinks.kafkaSink.batchSize=1

#a1.sinks.kafkaSink.serializer.class=kafka.serializer.StringEncoder
#a1.sinks.kafkaSink.type = org.apache.flume.sink.kafka.KafkaSink
#a1.sinks.kafkaSink.brokerList = 127.0.0.1:9092
#a1.sinks.kafkaSink.topic = webCount
#a1.sinks.kafkaSink.zookeeperConnect = 127.0.0.1:2181
